{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ุฌููุฉ ุณุฑูุนุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุงุจุฏุฃ ุฑุญูุชู ูุน ููุชุจุฉ ๐ค Transformers! ุณูุงุก ููุช ูุทูุฑูุง ุฃู ูุณุชุฎุฏููุง ุนุงุฏููุงุ ุณุชุณุงุนุฏู ูุฐู ุงูุฌููุฉ ุงูุณุฑูุนุฉ ุนูู ุงูุจุฏุก ูุณุชูุธูุฑ ูู ููููุฉ ุงุณุชุฎุฏุงู `pipeline()` ููุงุณุชูุชุงุฌุ ูุชุญููู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ููุนุงูุฌ ููุณุจู ูุน [AutoClass](https://huggingface.co/docs/transformers/main/ar/./model_doc/auto)ุ ูุชุฏุฑูุจ ูููุฐุฌ ุจุณุฑุนุฉ ุจุงุณุชุฎุฏุงู PyTorch ุฃู TensorFlow. ุฅุฐุง ููุช ูุจุชุฏุฆูุงุ ููุตู ุจุงูุงุทูุงุน ุนูู ุฏุฑูุณูุง ุฃู [ุงูุฏูุฑุฉ](https://huggingface.co/course/chapter1/1) ููุญุตูู ุนูู ุดุฑุญ ุฃูุซุฑ ุชุนูููุง ููููุงููู ุงูููุฏูุฉ ููุง.\n",
    "\n",
    "ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets evaluate accelerate\n",
    "```\n",
    "\n",
    "ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุซุจูุช ุฅุทุงุฑ ุนูู ุงูุชุนูู ุงูุขูู ุงูููุถู ูุฏูู:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุฎุท ุงูุฃูุงุจูุจ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ููุซู `pipeline()` ุฃุณูู ูุฃุณุฑุน ุทุฑููุฉ ูุงุณุชุฎุฏุงู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ููุงุณุชูุชุงุฌ. ููููู ุงุณุชุฎุฏุงู `pipeline()` ุฌุงูุฒูุง ููุนุฏูุฏ ูู ุงูููุงู ุนุจุฑ ุทุฑู ูุฎุชููุฉุ ูุงูุชู ูุธูุฑ ุจุนุถูุง ูู ุงูุฌุฏูู ุฃุฏูุงู:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ููุงุทูุงุน ุนูู ุงููุงุฆูุฉ ุงููุงููุฉ ููููุงู ุงููุชุงุญุฉุ ุฑุงุฌุน [ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ุงูุฎุงุตุฉ ุจุฎุท ุงูุฃูุงุจูุจ](https://huggingface.co/docs/transformers/main/ar/./main_classes/pipelines).\n",
    "\n",
    "</Tip>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "| **ุงููููุฉ**                     | **ุงููุตู**                                                                                              | **ุงูุทุฑููุฉ**    | **ูุนุฑู ุฎุท ุงูุฃูุงุจูุจ**                       |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
    "| ุชุตููู ุงููุต          | ุชุนููู ุชุณููุฉ ุฅูู ุชุณูุณู ูุต ูุนูู                                                                   | NLP             | pipeline(task=โsentiment-analysisโ)           |\n",
    "| ุชูููุฏ ุงููุต              | ุชูููุฏ ูุต ุจูุงุกู ุนูู ููุฌู ูุนูู                                                                                 | NLP             | pipeline(task=โtext-generationโ)              |\n",
    "| ุชูุฎูุต                | ุชูููุฏ ููุฎุต ูุชุณูุณู ูุต ุฃู ูุณุชูุฏ                                                         | NLP             | pipeline(task=โsummarizationโ)                |\n",
    "| ุชุตููู ุงูุตูุฑ         | ุชุนููู ุชุณููุฉ ูุตูุฑุฉ ูุนููุฉ                                                                                   | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โimage-classificationโ)         |\n",
    "| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ           | ุชุนููู ุชุณููุฉ ููู ุจูุณู ูุฑุฏู ูู ุงูุตูุฑุฉ (ูุฏุนู ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉุ ูุงููุฌููุฉุ ูุชุฌุฒุฆุฉ ูุซููุงุช) | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โimage-segmentationโ)           |\n",
    "| ุงูุชุดุงู ุงูุฃุดูุงุก             | ุงูุชูุจุค ุจุญุฏูุฏ ุงูุฃุดูุงุก ููุฆุงุชูุง ูู ุตูุฑุฉ ูุนููุฉ                                                | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โobject-detectionโ)             |\n",
    "| ุชุตููู ุงูุตูุช         | ุชุนููู ุชุณููุฉ ูุจูุงูุงุช ุตูุชูุฉ ูุนููุฉ                                                                            | ุตูุชู           | pipeline(task=โaudio-classificationโ)         |\n",
    "| ุงูุชุนุฑู ุนูู ุงูููุงู ุงูุชููุงุฆู | ูุณุฎ ุงูููุงู ุฅูู ูุต                                                                                  | ุตูุชู           | pipeline(task=โautomatic-speech-recognitionโ) |\n",
    "| ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงูุจุตุฑูุฉ    | ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุญูู ุงูุตูุฑุฉุ ูุน ุฅุนุทุงุก ุตูุฑุฉ ูุณุคุงู                                             | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task=โvqaโ)                          |\n",
    "| ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช  | ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุญูู ุงููุณุชูุฏุ ูุน ุฅุนุทุงุก ูุณุชูุฏ ูุณุคุงู                                        | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task=\"document-question-answering\")  |\n",
    "| ูุชุงุจุฉ ุชุนููู ุนูู ุงูุตูุฑุฉ             | ุฅูุดุงุก ุชุนููู ุนูู ุตูุฑุฉ ูุนููุฉ                                                                         | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task=\"image-to-text\")                |\n",
    "\n",
    "</div>\n",
    "ุงุจุฏุฃ ุจุฅูุดุงุก ูุซูู ูู `pipeline()` ูุชุญุฏูุฏ ุงููููุฉ ุงูุชู ุชุฑูุฏ ุงุณุชุฎุฏุงูู ููุง. ูู ูุฐุง ุงูุฏูููุ ุณุชุณุชุฎุฏู ุฎุท ุงูุฃูุงุจูุจ ููุชุญููู ุงููุตู ููููุฐุฌ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูููู `pipeline()` ุจุชูุฒูู ูุชุฎุฒูู ูุณุฎุฉ ุงุญุชูุงุทูุฉ ูู ูููุฐุฌ ุงูุชุฑุงุถู [ููุฏุฑุจ ูุณุจููุง](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) ููุนุงูุฌ ููุชุญููู ุงููุตู. ุงูุขู ููููู ุงุณุชุฎุฏุงู `classifier` ุนูู ุงููุต ุงููุณุชูุฏู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the ๐ค Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฅุฐุง ูุงู ูุฏูู ุฃูุซุฑ ูู ุฅุฏุฎุงู ูุงุญุฏุ ูู ุจุชูุฑูุฑ ุฅุฏุฎุงูุงุชู ููุงุฆูุฉ ุฅูู `pipeline()` ูุฅุฑุฌุงุน ูุงุฆูุฉ ูู ุงูููุงููุณ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label: POSITIVE, with score: 0.9998\n",
       "label: NEGATIVE, with score: 0.5309"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ๐ค Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูููู ูุฎุท ุงูุฃูุงุจูุจ ุฃูุถูุง ุฃู ูุชููู ุฎูุงู ูุฌููุนุฉ ุจูุงูุงุช ูุงููุฉ ูุฃู ูููุฉ ุชุฑูุฏูุง. ููุซุงู ุนูู ุฐููุ ุฏุนูุง ูุฎุชุงุฑ ุงูุชุนุฑู ุนูู ุงูููุงู ุงูุชููุงุฆู ููููุฉ ููุง:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ุตูุชูุฉ (ุฑุงุฌุน ุฏููู ุงูุจุฏุก ุงูุณุฑูุน ูู ๐ค Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู) ุงูุชู ุชุฑูุฏ ุงูุชููู ุฎูุงููุง. ุนูู ุณุจูู ุงููุซุงูุ ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุฌุจ ุงูุชุฃูุฏ ูู ุฃู ููุณ ุงูุฌูุฏุฉ ุงูุตูุชูุฉ (ูุนุฏู ุฃุฎุฐ ุงูุนููุงุช) ููุฌููุนุฉ ุงูุจูุงูุงุช ูุชุทุงุจู ูุน ูุนุฏู ุฃุฎุฐ ุงูุนููุงุช ุงูุฐู ุชู ุชุฏุฑูุจ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) ุนููู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุชู ุชุญููู ุงููููุงุช ุงูุตูุชูุฉ ูุฅุนุงุฏุฉ ุชุดููููุง ุชููุงุฆููุง ุนูุฏ ุงุณุชุฏุนุงุก ุงูุนููุฏ \"audio\".\n",
    "ุงุณุชุฎุฑุฌ ุงููุตูููุงุช ุงูููุฌูุฉ ุงูุฎุงู ูู ุฃูู 4 ุนููุงุช ููุฑุฑูุง ููุงุฆูุฉ ุฅูู ุฎุท ุงูุฃูุงุจูุจ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = speech_recognizer(dataset[:4][\"audio\"])\n",
    "print([d[\"text\"] for d in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุจุงููุณุจุฉ ููุฌููุนุงุช ุงูุจูุงูุงุช ุงููุจูุฑุฉ  ุงูุชู ุชุญุชูู ุนูู ูุฏุฎูุงุช ุถุฎูุฉ (ููุง ูู ุงูุญุงู ูู ุงูุจูุงูุงุช ุงูุตูุชูุฉ ุฃู ุงููุฑุฆูุฉ)ุ ููุถู ุชูุฑูุฑ ูููุฏ (generator) ุจุฏูุงู ูู ูุงุฆูุฉ ูุชุญููู ุฌููุน ุงููุฏุฎูุงุช ูู ุงูุฐุงูุฑุฉ ุฏูุนุฉ ูุงุญุฏุฉ. ุฑุงุฌุน [ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ุงูุฎุงุตุฉ ุจุฎุท ุงูุฃูุงุจูุจ](https://huggingface.co/docs/transformers/main/ar/./main_classes/pipelines) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุงุงุณุชุฎุฏู ูููุฐุฌูุง ููุฌุฒุฆูุง ุขุฎุฑูู ูู ุฎุท ุงูุฃูุงุจูุจ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูููู ูุฎุท ุงูุฃูุงุจูุจ `pipeline()` ุงุณุชูุนุงุจ ุฃู ูููุฐุฌ ูู [Hub](https://huggingface.co/models)ุ ููุง ูุณูู ุงูุชููู ูุน ุญุงูุงุช ุงูุงุณุชุฎุฏุงู ุงูุฃุฎุฑู. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ููุช ุชุฑูุฏ ูููุฐุฌูุง ูุงุฏุฑูุง ุนูู ุงูุชุนุงูู ูุน ุงููุต ุงููุฑูุณูุ ูุงุณุชุฎุฏู ุงูุนูุงูุงุช ุนูู Hub ูููุชุฑู ูููุฐุฌ ููุงุณุจ. ุชุนูุฏ ุงููุชูุฌุฉ ุงูุฃููู ุงููุฑุดุญุฉ ูููุฐุฌ BERT ูุชุนุฏุฏ ุงููุบุงุช [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) ุงูุฐู ุชู ุถุจุทู ูุณุจููุง ููุชุญููู ุงููุดุงุนุฑ ูุงูุฐู ููููู ุงุณุชุฎุฏุงูู ูููุต ุงููุฑูุณู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุงุณุชุฎุฏู `TFAutoModelForSequenceClassification` ู `AutoTokenizer` ูุชุญููู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ููุนุงูุฌุชู ุงููุฑุชุจุท ุจู (ูุฒูุฏ ูู ุงููุนูููุงุช ุญูู `TFAutoClass` ูู ุงููุณู ุงูุชุงูู):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุญุฏุฏ ุงููููุฐุฌ ูุงููุนุงูุฌ ูู `pipeline()`. ุงูุขู ููููู ุชุทุจูู `classifier` ุนูู ุงููุต ุงููุฑูุณู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7273}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trรจs heureux de vous prรฉsenter la bibliothรจque ๐ค Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฅุฐุง ูู ุชุฌุฏ ูููุฐุฌูุง ุฌุงูุฒูุง ููุงุณุจ ูููุชูุ ูุณุชุญุชุงุฌ ุฅูู ุถุจุท ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ุนูู ุจูุงูุงุชู. ุงุทูุน ุนูู [ุฏููู ุงูุถุจุท ุงูุฏููู](https://huggingface.co/docs/transformers/main/ar/./training) ููุชุนุฑู ุนูู ููููุฉ ุงูููุงู ุจุฐูู. ูุจุนุฏ ุถุจุท ูููุฐุฌู ุงูููุฏุฑุจ ูุณุจููุงุ ูุฑุฌู ูุฑุงุนุงุฉ [ุงููุดุงุฑูุฉ](https://huggingface.co/docs/transformers/main/ar/./model_sharing) ุงููููุฐุฌ ูุน ุงููุฌุชูุน ุนูู Hub ููุณุงุนุฏุฉ ุงูุฌููุน ูู ูุฌุงู ุงูุชุนูู ุงูุขูู! ๐ค"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูู ุงูุฎูููุฉุ ุชุนูู ูุฆุชุง `AutoModelForSequenceClassification` ู `AutoTokenizer` ูุนูุง ูุชุดุบูู ุฏุงูุฉ pipeline()  ุงูุฐู ุงุณุชุฎุฏูุชูุง ุฃุนูุงู. ุชุนุชุจุฑ [AutoClass](https://huggingface.co/docs/transformers/main/ar/./model_doc/auto) ุงุฎุชุตุงุฑูุง ูููู ุชููุงุฆููุง ุจุงุณุชุฑุฏุงุฏ ุจููุฉ ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ูู ุงุณูู ุฃู ูุณุงุฑู. ูู ูุง ุนููู ูุนูู ูู ุชุญุฏูุฏ ูุฆุฉ `AutoClass` ุงูููุงุณุจุฉ ููููุชู ููุฆุฉ ุงููุนุงูุฌุฉ ุงููุฑุชุจุทุฉ ุจูุง.\n",
    "\n",
    "ููุนุฏ ุฅูู ุงููุซุงู ูู ุงููุณู ุงูุณุงุจู ูููุฑู ููู ููููู ุงุณุชุฎุฏุงู `AutoClass` ูุชูุฑุงุฑ ูุชุงุฆุฌ ุฎุท ุงูุฃูุงุจูุจ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุงููุฌุฒุฆ ุงูุชููุงุฆู (AutoTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุชููู ุงููุฌุฒุฆ ูุณุคูููุฉ ุชุญููู ุงููุต ุฅูู ูุตูููุฉ ูู ุงูุฃุฑูุงู (ุฑููุฒ) ูููู ูููููุฐุฌ ููููุง ููุนุงูุฌุชูุง. ููุงู ููุงุนุฏ ูุชุนุฏุฏุฉ ุชุญูู ุนูููุฉ ุงูุชุฌุฒุฆุฉุ ุจูุง ูู ุฐูู ููููุฉ ุชูุณูู ูููุฉ ููุง ูู ุงููุณุชูู ุงูุฐู ูุฌุจ ุฃู ุชูุณูู ุงููููุงุช ุนูุฏู (ุชุนุฑู ุนูู ุงููุฒูุฏ ุญูู ุงููุนุงูุฌุฉ ูู [ููุฎุต ุงููุฌุฒุฆ](https://huggingface.co/docs/transformers/main/ar/./tokenizer_summary)). ุฃูู ุดูุก ูุฌุจ ุชุฐูุฑู ูู ุฃูู ุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุซูู ูููุฌุฒุฆ ุจููุณ ุงุณู ุงููููุฐุฌ ูุถูุงู ุงุณุชุฎุฏุงูู ูููุงุนุฏ ุงูุชุฌุฒุฆุฉ ููุณูุง ุงูุชู ุชู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูููุง.\n",
    "\n",
    "ูู ุจุชุญููู ุงููุฌุฒุฆ ุจุงุณุชุฎุฏุงู `AutoTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุฑุฑ ูุตู ุฅูู ุงููุฌุฒุฆ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ๐ค Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุนูุฏ ุงููุฌุฒุฆ ูุงููุณูุง ูุญุชูู ุนูู:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/ar/./glossary#input-ids): ุงูุชูุซููุงุช ุงูุฑูููุฉ ูุฑููุฒู.\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/ar/./glossary#attention-mask): ุชุดูุฑ ุฅูู ุงูุฑููุฒ ุงูุชู ูุฌุจ ุงูุงูุชุจุงู ุจูุง.\n",
    "\n",
    "ูููู ุงููุฌุฒุฆ ุฃูุถูุง ูุจูู ูุงุฆูุฉ ูู ุงููุฏุฎูุงุชุ ููููู ุจู \"ุญุดู\" ู\"ุชูุตูุฑ\" ุงููุต ูุฅุฑุฌุงุน ูุฏูุนุฉ ุจุทูู ููุญุฏ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ๐ค Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ุงุทูุน ุนูู [ุงูุฏููู ุงูุชูููุฏู ูููุนุงูุฌุฉ ุงููุณุจูุฉ](https://huggingface.co/docs/transformers/main/ar/./preprocessing) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงููุนุงูุฌุฉุ ูููููุฉ ุงุณุชุฎุฏุงู `AutoImageProcessor` ู `AutoFeatureExtractor` ู `AutoProcessor` ููุนุงูุฌุฉ ุงูุตูุฑ ูุงูุตูุช ูุงูุฅุฏุฎุงูุงุช ูุชุนุฏุฏุฉ ุงููุณุงุฆุท.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูููุฑ ๐ค Transformers ุทุฑููุฉ ุจุณูุทุฉ ูููุญุฏุฉ ูุชุญููู ูุซููุงุช ููุฏุฑุจุฉ ูุณุจููุง. ููุฐุง ูุนูู ุฃูู ููููู ุชุญููู `TFAutoModel` ูุซู ุชุญููู `AutoTokenizer`. ูุงููุฑู ุงููุญูุฏ ูู ุชุญุฏูุฏ `TFAutoModel` ุงูุตุญูุญ ูููููุฉ. ููุชุตููู ุงููุตู (ุฃู ุงูุชุณูุณูู)ุ ูุฌุจ ุชุญููู `TFAutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ุฑุงุฌุน [ููุฎุต ุงูููุงู](https://huggingface.co/docs/transformers/main/ar/./task_summary) ููููุงู ุงููุฏุนููุฉ ุจูุงุณุทุฉ ูุฆุฉ `AutoModel`.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ุงูุขูุ ูุฑุฑ ุฏูุนุฉ ุงููุฏุฎูุงุช ุงููุนุงูุฌุฉ ูุณุจููุง ูุจุงุดุฑุฉ ุฅูู ุงููููุฐุฌ. ููููู ุชูุฑูุฑ ุงููุตูููุงุช ููุง ูู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูููู ุงููููุฐุฌ ุจุฅุฎุฑุงุฌ ุงูุชูุดูุทุงุช ุงูููุงุฆูุฉ ูู ุณูุฉ `logits`. ุทุจู ุฏุงูุฉ softmax ุนูู `logits` ูุงุณุชุฑุฏุงุฏ ุงูุงุญุชูุงูุงุช:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ุชุฎุฑุฌ ุฌููุน ููุงุฐุฌ ๐ค Transformers (PyTorch ุฃู TensorFlow) ุงููุตูููุงุช *ูุจู* ุฏุงูุฉ ุงูุชูุดูุท ุงูููุงุฆูุฉ (ูุซู softmax) ูุฃู ุฏุงูุฉ ุงูุชูุดูุท ุงูููุงุฆูุฉ ุบุงูุจูุง ูุง ุชููู ูุฏูุฌุฉ ูุน ุฏุงูุฉ ุงูุฎุณุงุฑุฉ. ููุงุชุฌ ุงููููุฐุฌ ุนุจุงุฑุฉ ุนู ูุฆุงุช ุจูุงูุงุช ุฎุงุตุฉุ ูุฐูู ูุชู ุงุณุชููุงู ุณูุงุชูุง ุชููุงุฆููุง ูู IDE. ูุชุชุตุฑู ูุฎุฑุฌุงุช ุงููููุฐุฌ ูุซู ุฒูุฌ ูุฑุชุจ ุฃู ูุงููุณ (ููููู ุงูููุฑุณุฉ ุจุงุณุชุฎุฏุงู ุนุฏุฏ ุตุญูุญ ุ ุดุฑูุญุฉุ ุฃู ุณูุณูุฉ)ุ ููู ูุฐู ุงูุญุงูุฉุ ูุชู ุชุฌุงูู ุงูุณูุงุช ุงูุชู ุชุณุงูู None.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุญูุธ ุงููููุฐุฌ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุจูุฌุฑุฏ ุถุจุท ูููุฐุฌูุ ููููู ุญูุธู ูุน ุจุฑูุงูุฌ ุงูุชุฑููุฒ ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู `TFPreTrainedModel.save_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_save_directory = \"./tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "tf_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุนูุฏูุง ุชููู ูุณุชุนุฏูุง ูุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุฑุฉ ุฃุฎุฑูุ ุฃุนุฏ ุชุญูููู ุจุงุณุชุฎุฏุงู `TFPreTrainedModel.from_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูู ุงูููุฒุงุช ุงูุฑุงุฆุนุฉ ูู ๐ค Transformers ุงููุฏุฑุฉ ุนูู ุญูุธ ูููุฐุฌ ูุฅุนุงุฏุฉ ุชุญูููู ููููุฐุฌ PyTorch ุฃู TensorFlow. ูููู ุฃู ูุญูู ูุนุงูู `from_pt` ุฃู `from_tf` ุงููููุฐุฌ ูู ุฅุทุงุฑ ุนูู ุฅูู ุขุฎุฑ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุฅูุดุงุก ููุงุฐุฌ ูุฎุตุตุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ููููู ุชุนุฏูู ูุฆุฉ ุชูููู ุงููููุฐุฌ ูุชุบููุฑ ููููุฉ ุจูุงุก ุงููููุฐุฌ. ูุญุฏุฏ ุงูุชูููู ุณูุงุช ุงููููุฐุฌุ ูุซู ุนุฏุฏ ุงูุทุจูุงุช ุงููุฎููุฉ ุฃู ุฑุคูุณ ุงูุงูุชูุงู. ุชุจุฏุฃ ูู ุงูุตูุฑ ุนูุฏ ุชููุฆุฉ ูููุฐุฌ ูู ูุฆุฉ ุชูููู ูุฎุตุตุฉ. ูุชู ุชููุฆุฉ ุณูุงุช ุงููููุฐุฌ ุจุดูู ุนุดูุงุฆูุ ููุฌุจ ุชุฏุฑูุจ ุงููููุฐุฌ ูุจู ุงุณุชุฎุฏุงูู ููุญุตูู ุนูู ูุชุงุฆุฌ ุฐุงุช ูุนูู.\n",
    "\n",
    "ุงุจุฏุฃ ุจุงุณุชูุฑุงุฏ `AutoConfig`. ุซู ูู ุจุชุญููู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ุงูุฐู ุชุฑูุฏ ุชุนุฏููู. ุถูู `AutoConfig.from_pretrained()`. ููููู ุชุญุฏูุฏ ุงูุณูุฉ ุงูุชู ุชุฑูุฏ ุชุบููุฑูุงุ ูุซู ุนุฏุฏ ุฑุคูุณ ุงูุงูุชูุงู:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูู ุจุฅูุดุงุก ูููุฐุฌ ูู ุชููููู ุงููุฎุตุต ุจุงุณุชุฎุฏุงู `TFAutoModel.from_config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "my_model = TFAutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุงูู ูุธุฑุฉ ุนูู ุฏููู [ุฅูุดุงุก ุจููุฉ ูุฎุตุตุฉ](https://huggingface.co/docs/transformers/main/ar/./create_a_model) ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุจูุงุก ุงูุชููููุงุช ุงููุฎุตุตุฉ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุงููุฏุฑุจ - ุญููุฉ ุชุฏุฑูุจ ูุญุณูุฉ ูู PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฌููุน ุงูููุงุฐุฌ ุนุจุงุฑุฉ ุนู [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ููุงุณูุฉุ ูุฐุง ููููู ุงุณุชุฎุฏุงููุง ูู ุฃู ุญููุฉ ุชุฏุฑูุจ ูููุฐุฌูุฉ. ูู ุญูู ููููู ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจูุ ูููุฑ ๐ค Transformers ูุฆุฉ `Trainer` ูู PyTorchุ ูุงูุชู ุชุญุชูู ุนูู ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฃุณุงุณูุฉ ูุชุถูู ูุธุงุฆู ุฅุถุงููุฉ ูููุฒุงุช ูุซู ุงูุชุฏุฑูุจ ุงูููุฒุนุ ูุงูุฏูุฉ ุงููุฎุชูุทุฉุ ูุงููุฒูุฏ.\n",
    "\n",
    "ููููุง ููููุชูุ ุณุชููู ุนุงุฏุฉู ุจุชูุฑูุฑ ุงููุนููุงุช ุงูุชุงููุฉ ุฅูู `Trainer`:\n",
    "\n",
    "1. ุณุชุจุฏุฃ ุจู `PreTrainedModel` ุฃู [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "2. ุชุญุชูู `TrainingArguments` ุนูู ูุฑุท ูุนููุงุช ุงููููุฐุฌ ุงูุชู ููููู ุชุบููุฑูุง ูุซู ูุนุฏู ุงูุชุนููุ ูุญุฌู ุงูุฏูุนุฉุ ูุนุฏุฏ ุงูุนุตูุฑ ุงูุชู ูุฌุจ ุงูุชุฏุฑูุจ ุนูููุง. ูุชู ุงุณุชุฎุฏุงู ุงูููู ุงูุงูุชุฑุงุถูุฉ ุฅุฐุง ูู ุชุญุฏุฏ ุฃู ุญุฌุฌ ุชุฏุฑูุจ:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TrainingArguments\n",
    "\n",
    "   >>> training_args = TrainingArguments(\n",
    "   ...     output_dir=\"path/to/save/folder/\",\n",
    "   ...     learning_rate=2e-5,\n",
    "   ...     per_device_train_batch_size=8,\n",
    "   ...     per_device_eval_batch_size=8,\n",
    "   ...     num_train_epochs=2,\n",
    "   ... )\n",
    "   ```\n",
    "\n",
    "3. ูู ุจุชุญููู ูุฆุฉ ูุนุงูุฌุฉ ูุณุจูุฉ ูุซู ุจุฑูุงูุฌ ุงูุชุฑููุฒุ ุฃู ูุนุงูุฌ ุงูุตูุฑุ ุฃู ูุณุชุฎุฑุฌ ุงูููุฒุงุชุ ุฃู ุงููุนุงูุฌ:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "4. ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช:\n",
    "\n",
    "   ```py\n",
    "   >>> from datasets import load_dataset\n",
    "\n",
    "   >>> dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT\n",
    "   ```\n",
    "\n",
    "5. ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุชุฑููุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช:\n",
    "\n",
    "   ```py\n",
    "   >>> def tokenize_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])\n",
    "   ```\n",
    "\n",
    "   ุซู ูู ุจุชุทุจููู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุงุณุชุฎุฏุงู `map`:\n",
    "\n",
    "   ```py\n",
    "   >>> dataset = dataset.map(tokenize_dataset, batched=True)\n",
    "   ```\n",
    "\n",
    "6. `DataCollatorWithPadding` ูุฅูุดุงุก ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import DataCollatorWithPadding\n",
    "\n",
    "   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "   ```\n",
    "\n",
    "ุงูุขู ูู ุจุชุฌููุน ุฌููุน ูุฐู ุงููุฆุงุช ูู `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุนูุฏูุง ุชููู ูุณุชุนุฏูุงุ ุงุณุชุฏุนู `train()` ูุจุฏุก ุงูุชุฏุฑูุจ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ุจุงููุณุจุฉ ููููุงู - ูุซู ุงูุชุฑุฌูุฉ ุฃู ุงูุชูุฎูุต - ุงูุชู ุชุณุชุฎุฏู ูููุฐุฌ ุชุณูุณู ุฅูู ุชุณูุณูุ ุงุณุชุฎุฏู ูุฆุงุช `Seq2SeqTrainer` ู `Seq2SeqTrainingArguments` ุจุฏูุงู ูู ุฐูู.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ููููู ุชุฎุตูุต ุณููู ุญููุฉ ุงูุชุฏุฑูุจ ุนู ุทุฑูู ุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู ุงูุทุฑู ุฏุงุฎู `Trainer`. ูุณูุญ ูู ุฐูู ุจุชุฎุตูุต ููุฒุงุช ูุซู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ูุงููุญุณูุ ูุงููุฌุฏูู. ุฑุงุฌุน ูุฑุฌุน `Trainer` ููุชุนุฑู ุนูู ุงูุทุฑู ุงูุชู ูููู ุฅูุดุงุก ูุฆุงุช ูุฑุนูุฉ ูููุง.\n",
    "\n",
    "ูุงูุทุฑููุฉ ุงูุฃุฎุฑู ูุชุฎุตูุต ุญููุฉ ุงูุชุฏุฑูุจ ูู ุจุงุณุชุฎุฏุงู [ุงููุณุชุฏุนูุงุช](https://huggingface.co/docs/transformers/main/ar/./main_classes/callback). ููููู ุงุณุชุฎุฏุงู ุงููุณุชุฏุนูุงุช ููุชูุงูู ูุน ุงูููุชุจุงุช ุงูุฃุฎุฑู ููุฑุงูุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ููุฅุจูุงุบ ุนู ุงูุชูุฏู ุฃู ุฅููุงู ุงูุชุฏุฑูุจ ูุจูุฑูุง. ูุง ุชุนุฏู ุงููุณุชุฏุนูุงุช ุฃู ุดูุก ูู ุญููุฉ ุงูุชุฏุฑูุจ ููุณูุง. ูุชุฎุตูุต ุดูุก ูุซู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู `Trainer` ุจุฏูุงู ูู ุฐูู."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฌููุน ุงูููุงุฐุฌ ุนุจุงุฑุฉ ุนู [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ููุงุณูุฉุ ูุฐุง ูููู ุชุฏุฑูุจูุง ูู TensorFlow ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Keras. ูููุฑ ๐ค Transformers ุทุฑููุฉ `~TFPreTrainedModel.prepare_tf_dataset` ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุจุณูููุฉ ูู `tf.data.Dataset` ุญุชู ุชุชููู ูู ุงูุจุฏุก ูู ุงูุชุฏุฑูุจ ุนูู ุงูููุฑ ุจุงุณุชุฎุฏุงู ุฏุงูุชู `compile` ู`fit` ูู Keras.\n",
    "\n",
    "1. ุณุชุจุฏุฃ ุจู `TFPreTrainedModel` ุฃู [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model):\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "2. ูู ุจุชุญููู ูุฆุฉ ูุนุงูุฌุฉ ูุณุจูุฉ ูุซู ุจุฑูุงูุฌ ุงูุชุฑููุฒุ ุฃู ูุนุงูุฌ ุงูุตูุฑุ ุฃู ูุณุชุฎุฑุฌ ุงูููุฒุงุชุ ุฃู ุงููุนุงูุฌ:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "3. ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุชุฑููุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช:\n",
    "\n",
    "   ```py\n",
    "   >>> def tokenize_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])  # doctest: +SKIP\n",
    "   ```\n",
    "\n",
    "4. ูู ุจุชุทุจูู ุจุฑูุงูุฌ ุงูุชุฑููุฒ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุงุณุชุฎุฏุงู `map` ุซู ูุฑุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุจุฑูุงูุฌ ุงูุชุฑููุฒ ุฅูู `~TFPreTrainedModel.prepare_tf_dataset`. ููููู ุฃูุถูุง ุชุบููุฑ ุญุฌู ุงูุฏูุนุฉ ูุฎูุท ูุฌููุนุฉ ุงูุจูุงูุงุช ููุง ุฅุฐุง ุฃุฑุฏุช:\n",
    "\n",
    "   ```py\n",
    "   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP\n",
    "   >>> tf_dataset = model.prepare_tf_dataset(\n",
    "   ...     dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n",
    "   ... )  # doctest: +SKIP\n",
    "   ```\n",
    "\n",
    "5. ุนูุฏูุง ุชููู ูุณุชุนุฏูุงุ ููููู ุงุณุชุฏุนุงุก `compile` ู`fit` ูุจุฏุก ุงูุชุฏุฑูุจ. ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ูุฏููุง ุฏุงูุฉ ุฎุณุงุฑุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉ ุจุดูู ุงูุชุฑุงุถูุ ูุฐุง ูุฃูุช ูุณุช ุจุญุงุฌุฉ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:\n",
    "\n",
    "   ```py\n",
    "   >>> from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "   >>> model.compile(optimizer='adam')  # ูุง ุชูุฌุฏ ูุณูุทุฉ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ!\n",
    "   >>> model.fit(tf_dataset)  # doctest: +SKIP\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ูุงุฐุง ุจุนุฏุ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุงูุขู ุจุนุฏ ุฃู ุฃูููุช ุงูุฌููุฉ ุงูุณุฑูุนุฉ ูู ๐ค Transformersุ ุฑุงุฌุน ุฃุฏูุชูุง ููุนุฑูุฉ ููููุฉ ุงูููุงู ุจุฃุดูุงุก ุฃูุซุฑ ุชุญุฏูุฏูุง ูุซู ูุชุงุจุฉ ูููุฐุฌ ูุฎุตุตุ ูุถุจุท ูููุฐุฌ ูุณุจู ุงูุชุฏุฑูุจ ููููุฉ ูุนููุฉุ ูููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ุจุงุณุชุฎุฏุงู ูุต ุจุฑูุฌู. ุฅุฐุง ููุช ููุชููุง ุจูุนุฑูุฉ ุงููุฒูุฏ ุนู ุงูููุงููู ุงูุฃุณุงุณูุฉ ูู ๐ค Transformersุ ูุงุญุตู ุนูู ููุฌุงู ูู ุงููููุฉ ูุงุทูุน ุนูู ุฃุฏูุฉ ุงูููุงููู ุงูุฎุงุตุฉ ุจูุง!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
