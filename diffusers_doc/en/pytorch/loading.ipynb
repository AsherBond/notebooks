{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion models consists of multiple components like UNets or diffusion transformers (DiTs), text encoders, variational autoencoders (VAEs), and schedulers. The [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) wraps all of these components into a single easy-to-use API without giving up the flexibility to modify it's components.\n",
    "\n",
    "This guide will show you how to load a [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) is a base pipeline class that automatically selects and returns an instance of a model's pipeline subclass, like [QwenImagePipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/qwenimage#diffusers.QwenImagePipeline), by scanning the `model_index.json` file for the class name.\n",
    "\n",
    "Pass a model id to [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) to load a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model has a specific pipeline subclass that inherits from [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline). A subclass usually has a narrow focus and are task-specific. See the table below for an example.\n",
    "\n",
    "| pipeline subclass | task |\n",
    "|---|---|\n",
    "| [QwenImagePipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/qwenimage#diffusers.QwenImagePipeline) | text-to-image |\n",
    "| [QwenImageImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/qwenimage#diffusers.QwenImageImg2ImgPipeline) | image-to-image |\n",
    "| [QwenImageInpaintPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/qwenimage#diffusers.QwenImageInpaintPipeline) | inpaint |\n",
    "\n",
    "You could use the subclass directly by passing a model id to [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import QwenImagePipeline\n",
    "\n",
    "pipeline = QwenImagePipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines can also be run locally. Use [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download) to download a model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"Qwen/Qwen-Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is downloaded to your [cache](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../installation#cache). Pass the folder path to [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import QwenImagePipeline\n",
    "\n",
    "pipeline = QwenImagePipeline.from_pretrained(\n",
    "  \"path/to/your/cache\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) method won't download files from the Hub when it detects a local path. But this also means it won't download and cache any updates that have been made to the model either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `torch_dtype` argument in [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) to load a model with a specific data type. This allows you to load different models in different precisions. For example, loading a large transformer model in half-precision reduces the memory required.\n",
    "\n",
    "Pass the data type for each model as a dictionary to `torch_dtype`. Use the `default` key to set the default data type. If a model isn't in the dictionary and `default` isn't provided, it is loaded in full precision (`torch.float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import QwenImagePipeline\n",
    "\n",
    "pipeline = QwenImagePipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\",\n",
    "  torch_dtype={\"transformer\": torch.bfloat16, \"default\": torch.float16},\n",
    ")\n",
    "print(pipeline.transformer.dtype, pipeline.vae.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to use a dictionary if you're loading all the models in the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import QwenImagePipeline\n",
    "\n",
    "pipeline = QwenImagePipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "print(pipeline.transformer.dtype, pipeline.vae.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `device_map` argument determines individual model or pipeline placement on an accelerator like a GPU. It is especially helpful when there are multiple GPUs.\n",
    "\n",
    "Diffusers currently provides three options to `device_map`, `\"cuda\"`, `\"balanced\"` and `\"auto\"`. Refer to the table below to compare the three placement strategies.\n",
    "\n",
    "| parameter | description |\n",
    "|---|---|\n",
    "| `\"cuda\"` | places model or pipeline on CUDA device |\n",
    "| `\"balanced\"` | evenly distributes model or pipeline on all GPUs |\n",
    "| `\"auto\"` | distribute model from fastest device first to slowest |\n",
    "\n",
    "Use the `max_memory` argument in [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) to allocate a maximum amount of memory to use on each device. By default, Diffusers uses the maximum amount available.\n",
    "\n",
    "<hfoptions id=\"device_map\">\n",
    "<hfoption id=\"pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\", \n",
    "  torch_dtype=torch.bfloat16,\n",
    "  device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</hfoption>\n",
    "<hfoption id=\"individual model\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoModel\n",
    "\n",
    "max_memory = {0: \"16GB\", 1: \"16GB\"}\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    \"Qwen/Qwen-Image\", \n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16\n",
    "    device_map=\"cuda\",\n",
    "    max_memory=max_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</hfoption>\n",
    "</hfoptions>\n",
    "\n",
    "The `hf_device_map` attribute allows you to access and view the `device_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.hf_device_map)\n",
    "# {'unet': 1, 'vae': 1, 'safety_checker': 0, 'text_encoder': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset a pipeline's `device_map` with the [reset_device_map()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.reset_device_map) method. This is necessary if you want to use methods such as `.to()`, [enable_sequential_cpu_offload()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload), and [enable_model_cpu_offload()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.reset_device_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large models are often [sharded](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../training/distributed_inference#model-sharding) into smaller files so that they are easier to load. Diffusers supports loading shards in parallel to speed up the loading process.\n",
    "\n",
    "Set `HF_ENABLE_PARALLEL_LOADING` to `\"YES\"` to enable parallel loading of shards.\n",
    "\n",
    "The `device_map` argument should be set to `\"cuda\"` to pre-allocate a large chunk of memory based on the model size. This substantially reduces model load time because warming up the memory allocator now avoids many smaller calls to the allocator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "os.environ[\"HF_ENABLE_PARALLEL_LOADING\"] = \"YES\"\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"Wan-AI/Wan2.2-I2V-A14B-Diffusers\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing models in a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) is flexible and accommodates loading different models or schedulers. You can experiment with different schedulers to optimize for generation speed or quality, and you can replace models with more performant ones.\n",
    "\n",
    "The example below swaps the default scheduler to generate higher quality images and a more stable VAE version. Pass the `subfolder` argument in [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview#diffusers.SchedulerMixin.from_pretrained) to load the scheduler to the correct subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, HeunDiscreteScheduler, AutoModel\n",
    "\n",
    "scheduler = HeunDiscreteScheduler.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\"\n",
    ")\n",
    "vae = AutoModel.from_pretrained(\n",
    "  \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "  scheduler=scheduler,\n",
    "  vae=vae,\n",
    "  torch_dtype=torch.float16,\n",
    "  device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing models in multiple pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with multiple pipelines that use the same model, the [from_pipe()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe) method enables reusing a model instead of reloading it each time. This allows you to use multiple pipelines without increasing memory usage.\n",
    "\n",
    "Memory usage is determined by the pipeline with the highest memory requirement regardless of the number of pipelines.\n",
    "\n",
    "The example below loads a pipeline and then loads a second pipeline with [from_pipe()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe) to use [perturbed-attention guidance (PAG)](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../api/pipelines/pag) to improve generation quality.\n",
    "\n",
    "> [!WARNING]\n",
    "> Use [AutoPipelineForText2Image](https://huggingface.co/docs/diffusers/main/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image) because [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) doesn't support PAG. Refer to the [AutoPipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../tutorials/autopipeline) docs to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "pipeline_sdxl = AutoPipelineForText2Image.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, device_map=\"cuda\"\n",
    ")\n",
    "prompt = \"\"\"\n",
    "cinematic film still of a cat sipping a margarita in a pool in Palm Springs, California\n",
    "highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\n",
    "\"\"\"\n",
    "image = pipeline_sdxl(prompt).images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "# Max memory reserved: 10.47 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `enable_pag=True` in the second pipeline to enable PAG. The second pipeline uses the same amount of memory because it shares model weights with the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pipe(\n",
    "  pipeline_sdxl, enable_pag=True\n",
    ")\n",
    "prompt = \"\"\"\n",
    "cinematic film still of a cat sipping a margarita in a pool in Palm Springs, California\n",
    "highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\n",
    "\"\"\"\n",
    "image = pipeline(prompt).images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "# Max memory reserved: 10.47 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!WARNING]\n",
    "> Pipelines created by [from_pipe()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe) share the same models and *state*. Modifying the state of a model in one pipeline affects all the other pipelines that share the same model.\n",
    "\n",
    "Some methods may not work correctly on pipelines created with [from_pipe()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pipe). For example, [enable_model_cpu_offload()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload) relies on a unique model execution order, which may differ in the new pipeline. To ensure proper functionality, reapply these methods on the new pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusers provides a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) for older Stable Diffusion models to prevent generating harmful content. It screens the generated output against a set of hardcoded harmful concepts.\n",
    "\n",
    "If you want to disable the safety checker, pass `safety_checker=None` in [from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"stable-diffusion-v1-5/stable-diffusion-v1-5\", safety_checker=None\n",
    ")\n",
    "\"\"\"\n",
    "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
